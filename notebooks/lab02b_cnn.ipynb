{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlH0lCOttCs5"
   },
   "source": [
    "<img src=\"https://fsdl.me/logo-720-dark-horizontal\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUPRHaeetRnT"
   },
   "source": [
    "# Lab 02b: Training a CNN on Synthetic Handwriting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bry3Hr-PcgDs"
   },
   "source": [
    "### What You Will Learn\n",
    "\n",
    "- Fundamental principles for building neural networks with convolutional components\n",
    "- How to generate synthetic data in PyTorch\n",
    "- How to use Lightning's training framework via a CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vs0LXXlCU6Ix"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkQiK7lkgeXm"
   },
   "source": [
    "If you're running this notebook on Google Colab,\n",
    "the cell below will run full environment setup.\n",
    "\n",
    "It should take about three minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env FSDL_REPO=fsdl-text-recognizer-2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sVx7C7H0PIZC"
   },
   "outputs": [],
   "source": [
    "lab_idx = 2\n",
    "\n",
    "if \"bootstrap\" not in locals() or bootstrap.run:\n",
    "    # path management for Python\n",
    "    pythonpath, = !echo $PYTHONPATH\n",
    "    if \".\" not in pythonpath.split(\":\"):\n",
    "        pythonpath = \".:\" + pythonpath\n",
    "        %env PYTHONPATH={pythonpath}\n",
    "        !echo $PYTHONPATH\n",
    "\n",
    "    # get both Colab and local notebooks into the same state\n",
    "    !wget --quiet https://fsdl.me/gist-bootstrap -O bootstrap.py\n",
    "    import bootstrap\n",
    "\n",
    "    # change into the lab directory\n",
    "    bootstrap.change_to_lab_dir(lab_idx=lab_idx)\n",
    "\n",
    "    # allow \"hot-reloading\" of modules\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "    bootstrap.run = False  # change to True re-run setup\n",
    "    \n",
    "!pwd\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZN4bGgsgWc_"
   },
   "source": [
    "# Why convolutions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic neural networks,\n",
    "multi-layer perceptrons,\n",
    "are built by alternating\n",
    "parameterized linear transformations\n",
    "with non-linear transformations.\n",
    "\n",
    "This combination is capable of expressing\n",
    "[functions of arbitrary complexity](http://neuralnetworksanddeeplearning.com/chap4.html),\n",
    "so long as those functions\n",
    "take in fixed-size arrays and return fixed-size arrays.\n",
    "\n",
    "But not all functions have that type signature.\n",
    "\n",
    "For example, we might want to identify the content of images\n",
    "that have different sizes.\n",
    "Without gross hacks,\n",
    "an MLP won't be able to solve this problem,\n",
    "even though it seems simple enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import IPython.display as display\n",
    "\n",
    "randsize = 10 ** (random.random() * 2 + 1)\n",
    "\n",
    "Url = \"https://fsdl-public-assets.s3.us-west-2.amazonaws.com/emnist/U.png\"\n",
    "\n",
    "# run multiple times to display the same image at different sizes\n",
    "#  the content of the image remains unambiguous\n",
    "display.Image(url=Url, width=randsize, height=randsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relatedly, MLPs are too general to be efficient:\n",
    "they use an unstructured matrix\n",
    "to transform their inputs,\n",
    "but most of the data we want to apply them to is highly structured.\n",
    "\n",
    "Being able to represent arbitrary functions is nice, but\n",
    "[most functions are monstrous outrages against common sense](https://en.wikipedia.org/wiki/Weierstrass_function#Density_of_nowhere-differentiable_functions).\n",
    "It is useful to encode some of our assumptions\n",
    "about the kinds of functions we might want to learn\n",
    "from our data into our model's architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most common types of structure in data is \"locality\" --\n",
    "the most relevant information for understanding or predicting a pixel\n",
    "is a small number of pixels around it.\n",
    "\n",
    "Locality is a fundamental feature of the physical world,\n",
    "so it shows up in data drawn from physical observations,\n",
    "like photographs and audio recordings.\n",
    "\n",
    "Locality means most meaningful linear transformations of our input\n",
    "only have large weights in a small number of entries that are close to one another,\n",
    "rather than having weights in all entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "generic_linear_transform = torch.randn(8, 1)\n",
    "print(\"generic:\", generic_linear_transform, sep=\"\\n\")\n",
    "\n",
    "local_linear_transform = torch.tensor([\n",
    "    [0], [0], [0], [random.random()], [random.random()], [random.random()], [0], [0]])\n",
    "print(\"local:\", local_linear_transform, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another type of structure commonly observed is \"translation invariance\" --\n",
    "the top-left pixel is not meaningfully different from the bottom-right pixel\n",
    "or a pixel in the middle of the image.\n",
    "Relative relationships matter more than absolute relationships.\n",
    "\n",
    "Translation invariance arises in images because there is generally no privileged\n",
    "vantage point for taking the image.\n",
    "We could just as easily have taken the image while standing a few feet to the left or right.\n",
    "\n",
    "Translation invariance means that a linear transformation that is meaningful at one position\n",
    "in our input is likely to be meaningful at all other points.\n",
    "We can learn something about a linear transformation from a datapoint where it is useful\n",
    "in the bottom-left and then apply it to another datapoint where it's useful in the top-right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_linear_transform = torch.arange(8)[:, None]\n",
    "print(\"generic:\", generic_linear_transform, sep=\"\\n\")\n",
    "\n",
    "invariant_linear_transform = torch.stack([torch.roll(generic_linear_transform[:, 0], ii) for ii in range(8)], dim=1)\n",
    "print(\"translation invariant:\", invariant_linear_transform, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear transformation that is translation invariant is called a _convolution_.\n",
    "\n",
    "If the weights of that linear transformation are mostly zero\n",
    "except for a few that are close to one another,\n",
    "that convolution is said to have a _kernel_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = torch.nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3)  # the equivalent of \n",
    "\n",
    "conv_layer.weight  # aka kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using normal matrix multiplication to apply the kernel to the input,\n",
    "we repeatedly apply that kernel over and over again,\n",
    "\"sliding\" it over the input to produce an output.\n",
    "\n",
    "Every convolution kernel has an equivalent matrix form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_kernel_as_vector = torch.hstack([conv_layer.weight[0][0], torch.zeros(5)])\n",
    "conv_layer_as_matrix = torch.stack([torch.roll(conv_kernel_as_vector, ii) for ii in range(8)], dim=0)\n",
    "print(\"convolution matrix:\", conv_layer_as_matrix, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <small> Under the hood, the actual operation that implements the application of a convolutional kernel\n",
    "need not look like either of these\n",
    "(common approaches include\n",
    "[Winograd-type algorithms](https://arxiv.org/abs/1509.09308)\n",
    "and [Fast Fourier Transform-based algorithms](https://arxiv.org/abs/1312.5851)) </small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though they may seem somewhat arbitrary and technical,\n",
    "convolutions are actually a deep and fundamental bit of mathematics and computer science.\n",
    "Fundamental as in\n",
    "[closely related to the multiplication algorithm we learn as children](https://charlesfrye.github.io/math/2019/02/20/multiplication-convoluted-part-one.html)\n",
    "and deep as in\n",
    "[closely related to the Fourier transform](https://math.stackexchange.com/questions/918345/fourier-transform-as-diagonalization-of-convolution).\n",
    "They can show up wherever there is a sum over paths,\n",
    "as is common in dynamic programming.\n",
    "\n",
    "See Chris Olah's blog series\n",
    "([1](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/),\n",
    "[2](https://colah.github.io/posts/2014-07-Understanding-Convolutions/),\n",
    "[3](https://colah.github.io/posts/2014-12-Groups-Convolution/))\n",
    "for a friendly introduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying convolutions to handwritten characters: `CNN` and `EMNIST`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import text_recognizer.models\n",
    "\n",
    "\n",
    "text_recognizer.models.CNN??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need a data config to instantiate, so we need data.\n",
    "\n",
    "Let's bring in a slightly more fun dataset than MNIST: _EMNIST_,\n",
    "which has letters\n",
    "\n",
    "We've built a a PyTorch Lightning `DataModule`\n",
    "to encapsualte all the code needed to get this dataset ready to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import text_recognizer.data\n",
    "\n",
    "emnist = text_recognizer.data.EMNIST()  # configure\n",
    "emnist.prepare_data()  # download, save to disk\n",
    "emnist.setup()  # create torch.utils.data.Datasets, do train/val split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class comes with pretty printing\n",
    "for quick examination of metadata and basic descriptive statistics.\n",
    "\n",
    "> <small> You can add pretty printing to your own Python classes by writing\n",
    "`__str__` or `__repr__` methods for them.\n",
    "The former is generally expected to be human-readable,\n",
    "while the latter is generally expected to be machine-readable;\n",
    "we've broken with that custom here. </small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we've run `.prepare_data` and `.setup`,\n",
    "we can expect that this `DataModule` is ready to provide a `DataLoader`\n",
    "if we invoke the right method --\n",
    "sticking to the PyTorch Lightning API brings these kinds of convenient guarantees\n",
    "even when we're not using the `Trainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = next(iter(emnist.train_dataloader()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to inspect random elements of this batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "idx = random.randint(0, len(xs))\n",
    "\n",
    "print(emnist.mapping[ys[idx]])\n",
    "wandb.Image(xs[idx]).image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting convolutions in a `torch.nn.Module`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have the data,\n",
    "we have a `data_config`\n",
    "and can instantiate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = emnist.config()\n",
    "\n",
    "cnn = text_recognizer.models.CNN(data_config)\n",
    "cnn  # reveals the sub-nn.Modules attached to our nn.Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the `.forward` method to see how these `nn.Module`s are used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.forward??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classic \"LeNet\" architecture for labeling images:\n",
    "Convolutions followed by non-linearities,\n",
    "with (maximum) pooling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then `torch.flatten` to get vectors to pass into an MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bugbear of convolutional networks: shape inference.\n",
    "\n",
    "Easiest thing is to just keep the shape the same within a block,\n",
    "then downsample between.\n",
    "Otherwise the\n",
    "[arithmetic gets pretty hairy](https://arxiv.org/abs/1603.07285).\n",
    "\n",
    "That's what we do.\n",
    "\n",
    "As shapes change, so does the amount of GPU memory taken up by the tensors.\n",
    "Keeping sizes fixed within a block removes one axis of variation\n",
    "for an important resource.\n",
    "After applying our pooling layer,\n",
    "we can just increase the number of kernels by the same factor\n",
    "to keep memory constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the \"parameter efficiency\" of convolutional layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[p.numel() for p in cnn.parameters()]  # conv weight + bias, conv weight + bias, fc weight + bias, fc weight + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest layer is typically between the convolution component and the fc component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biggest_layer = [p for p in cnn.parameters() if p.numel() == max(p.numel() for p in cnn.parameters())][0]\n",
    "biggest_layer.shape, cnn.fc_input_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this layer dominates the cost of storing the network on disk.\n",
    "\n",
    "but that doesn't mean it dominates compute costs.\n",
    "\n",
    "convolutions are parameter efficient, but those parameters are used many times,\n",
    "so the total number of computations done by the layer can be higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of multiplications per input == nparams\n",
    "cnn.fc1.weight.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_conv_multiplications(kernel_shape, input_size=(64, 28, 28)):\n",
    "    num_kernel_elements = 1\n",
    "    for dimension in kernel_shape[-3:]:\n",
    "        num_kernel_elements *= dimension\n",
    "    num_input_channels, num_kernels = input_size[0], kernel_shape[0]\n",
    "    num_spatial_applications = ((input_size[1] - kernel_shape[-2]) * (input_size[2] - kernel_shape[-1]))\n",
    "    mutliplications_per_kernel = num_spatial_applications * num_kernel_elements * num_input_channels\n",
    "    return mutliplications_per_kernel * num_kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_conv_multiplications(cnn.conv2.conv.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_conv_multiplications(cnn.conv2.conv.weight.shape) // cnn.fc1.weight.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your compute hardware's characteristics --\n",
    "are you memory constrained, like when transferring a model \"over the wire\" to a browser\n",
    "or are you compute constrained, like when running a model on a low-power edge device? --\n",
    "either of these components could become the critical bottleneck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the CNN on EMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the CLI, getting `--help`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating lines of text from handwritten characters: `EMNISTLines`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem we're solving not obviously useful:\n",
    "it's individual characters.\n",
    "\n",
    "Would need a component to first pull out the characters.\n",
    "\n",
    "Not the ethos of deep learning, which operates \"end-to-end\".\n",
    "\n",
    "Let's kick the realism up a notch by building lines of text out of our characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not yet fully realistic, even for single lines --\n",
    "characters don't overlap, they're all the same length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying CNNs to handwritten text: `LineCNNSimple`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeatedly apply the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_recognizer.models.LineCNNSimple??"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lab02a_lightning.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "0f056848cf5d2396a4970b625f23716aa539c2ff5334414c1b5d98d7daae66f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
